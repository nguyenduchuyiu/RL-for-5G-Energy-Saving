{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RL-for-5G-Energy-Saving' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nguyenduchuyiu/RL-for-5G-Energy-Saving.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving\n"
     ]
    }
   ],
   "source": [
    "%cd RL-for-5G-Energy-Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/energy_agent/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: True\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model_cur.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "qos_grad_coeff: 10.0\n",
    "energy_grad_coeff: 5.0\n",
    "cpu_grad_coeff: 1.0\n",
    "prb_grad_coeff: 1.0\n",
    "stability_penalty: -10.0\n",
    "energy_consumption_penalty: -100.0\n",
    "violation_penalty: -50.0\n",
    "baseline_reward: 10.0\n",
    "\n",
    "entropy_coef: 0.01\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 64\n",
    "buffer_size: 4\n",
    "n_envs: 4\n",
    "hidden_dim: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/dense_urban.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/dense_urban.json\n",
    "{\n",
    "  \"name\": \"Dense Urban\",\n",
    "  \"description\": \"3GPP Dense urban with macro and micro cells\",\n",
    "  \"deploymentScenario\": \"dense_urban\",\n",
    "  \n",
    "  \"carrierFrequency\": 4e9,\n",
    "  \"systemBandwidth\": 200e6,\n",
    "  \"layout\": \"two_layer\",\n",
    "  \"isd\": 200,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"Uniform/macro\",\n",
    "  \"ueSpeed\": 3,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 10,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 5120,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 20,\n",
    "  \"peakHourMultiplier\": 1.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/urban_macro.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/urban_macro.json\n",
    "    {\n",
    "  \"name\": \"3GPP Urban Macro\",\n",
    "  \"description\": \"3GPP Urban macro deployment scenario with large cells and continuous coverage\",\n",
    "  \"deploymentScenario\": \"urban_macro\",\n",
    "  \n",
    "  \"carrierFrequency\": 2e9,\n",
    "  \"systemBandwidth\": 100e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 500,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 30,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 0,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 25,\n",
    "  \"peakHourMultiplier\": 1.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/rural.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/rural.json\n",
    "{\n",
    "  \"name\": \"3GPP Rural Macro\",\n",
    "  \"description\": \"3GPP Rural deployment scenario with wide area coverage for high speed vehicles\",\n",
    "  \"deploymentScenario\": \"rural\",\n",
    "  \n",
    "  \"carrierFrequency\": 700e6,\n",
    "  \"systemBandwidth\": 20e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 1732,\n",
    "  \n",
    "  \"numSites\": 19,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 35,\n",
    "  \"cellRadius\": 1000,\n",
    "  \n",
    "  \"numUEs\": 100,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 120,\n",
    "  \"indoorRatio\": 0.5,\n",
    "  \"outdoorSpeed\": 120,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1200,\n",
    "  \"idlePower\": 300,\n",
    "  \n",
    "  \"simTime\": 0,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -115,\n",
    "  \"rsrpTargetThreshold\": -105,\n",
    "  \"rsrpMeasurementThreshold\": -120,\n",
    "  \"dropCallThreshold\": 2,\n",
    "  \"latencyThreshold\": 100,\n",
    "  \"cpuThreshold\": 90,\n",
    "  \"prbThreshold\": 90,\n",
    "  \n",
    "  \"trafficLambda\": 10,\n",
    "  \"peakHourMultiplier\": 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/anaconda3/envs/mira/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "\n",
      "=== Running Benchmark Suite (3 scenarios) ===\n",
      "Scenarios directory: app/train_scenarios/\n",
      "Using 4 parallel environments per scenario\n",
      "Loaded scenarios: ['dense_urban', 'rural', 'urban_macro']\n",
      "\n",
      "\n",
      "--- Scenario 1/3: dense_urban ---\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "Created 4 parallel environments\n",
      "  Scenario: dense_urban\n",
      "  Action dim: 21\n",
      "  Obs dim: 280\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "2025-10-27 12:46:42,468 - PPOAgent - INFO - PPO Agent initialized: 21 cells, 300 UEs\n",
      "2025-10-27 12:46:42,468 - PPOAgent - INFO - State dim: 1445, Action dim: 100\n",
      "2025-10-27 12:46:42,468 - PPOAgent - INFO - Device: cuda\n",
      "2025-10-27 12:46:42,468 - PPOAgent - INFO - No checkpoint found at app/energy_agent/models/ppo_model_cur.pth\n",
      "Created 7 sites for dense_urban scenarioCreated 7 sites for dense_urban scenario\n",
      "\n",
      "Configuring cells for dense_urban scenario...Configuring cells for dense_urban scenario...\n",
      "\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Created 7 sites for dense_urban scenarioCreated 7 sites for dense_urban scenario\n",
      "\n",
      "Configuring cells for dense_urban scenario...Configuring cells for dense_urban scenario...\n",
      "\n",
      "Configured 21 cells for dense_urban scenarioConfigured 21 cells for dense_urban scenario\n",
      "\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "===================Starting scenario===================\n",
      "Starting episode: 1\n",
      "Training with 4 parallel environments...\n",
      "Total steps: 5120\n",
      "total_reward:  -124.73362 \n",
      "drop_improvement:  -10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -114.73362 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:47:06,992 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=7625.2275, Entropy=0.9189\n",
      "2025-10-27 12:47:07,025 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0019, Critic loss=7502.4839, Entropy=0.9177\n",
      "2025-10-27 12:47:07,059 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0038, Critic loss=7397.6372, Entropy=0.9164\n",
      "2025-10-27 12:47:07,090 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0057, Critic loss=7310.0410, Entropy=0.9150\n",
      "2025-10-27 12:47:07,126 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0076, Critic loss=7233.1826, Entropy=0.9136\n",
      "2025-10-27 12:47:07,164 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0096, Critic loss=7171.9072, Entropy=0.9119\n",
      "2025-10-27 12:47:07,204 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0116, Critic loss=7118.0122, Entropy=0.9100\n",
      "2025-10-27 12:47:07,242 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0136, Critic loss=7068.7544, Entropy=0.9078\n",
      "2025-10-27 12:47:07,243 - PPOAgent - INFO - Training completed: Actor loss=-0.0136, Critic loss=7068.7544\n",
      "Starting episode: 2\n",
      "total_reward:  -101.39232 \n",
      "drop_improvement:  2.50425 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  -0.84211 \n",
      "prb_improvement:  -0.84211 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -102.21236 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:47:21,309 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=13799.4619, Entropy=0.9056\n",
      "2025-10-27 12:47:21,334 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0003, Critic loss=13723.1875, Entropy=0.9038\n",
      "2025-10-27 12:47:21,359 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0008, Critic loss=13654.6777, Entropy=0.9024\n",
      "2025-10-27 12:47:21,384 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0016, Critic loss=13597.8564, Entropy=0.9012\n",
      "2025-10-27 12:47:21,408 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0026, Critic loss=13551.6309, Entropy=0.9002\n",
      "2025-10-27 12:47:21,433 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0036, Critic loss=13509.4092, Entropy=0.8994\n",
      "2025-10-27 12:47:21,457 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0048, Critic loss=13471.7119, Entropy=0.8987\n",
      "2025-10-27 12:47:21,481 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0060, Critic loss=13438.6602, Entropy=0.8981\n",
      "2025-10-27 12:47:21,482 - PPOAgent - INFO - Training completed: Actor loss=-0.0060, Critic loss=13438.6602\n",
      "Starting episode: 3\n",
      "total_reward:  -60.93879 \n",
      "drop_improvement:  6.87893 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -67.81772 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:47:32,635 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0000, Critic loss=7787.7051, Entropy=0.8983\n",
      "2025-10-27 12:47:32,659 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0003, Critic loss=7763.9419, Entropy=0.8975\n",
      "2025-10-27 12:47:32,683 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0009, Critic loss=7741.7588, Entropy=0.8965\n",
      "2025-10-27 12:47:32,707 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0017, Critic loss=7722.5215, Entropy=0.8955\n",
      "2025-10-27 12:47:32,732 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0028, Critic loss=7705.8750, Entropy=0.8945\n",
      "2025-10-27 12:47:32,756 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0040, Critic loss=7692.2993, Entropy=0.8935\n",
      "2025-10-27 12:47:32,781 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0053, Critic loss=7680.8535, Entropy=0.8924\n",
      "2025-10-27 12:47:32,805 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0067, Critic loss=7671.2148, Entropy=0.8912\n",
      "2025-10-27 12:47:32,805 - PPOAgent - INFO - Training completed: Actor loss=-0.0067, Critic loss=7671.2148\n",
      "Starting episode: 4\n",
      "total_reward:  -65.63642 \n",
      "drop_improvement:  0.36355 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -65.99997 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:47:44,153 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0000, Critic loss=8049.9658, Entropy=0.8899\n",
      "2025-10-27 12:47:44,180 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0002, Critic loss=8039.9297, Entropy=0.8888\n",
      "2025-10-27 12:47:44,204 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0008, Critic loss=8031.0151, Entropy=0.8878\n",
      "2025-10-27 12:47:44,228 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0016, Critic loss=8022.6484, Entropy=0.8869\n",
      "2025-10-27 12:47:44,252 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0026, Critic loss=8014.6689, Entropy=0.8862\n",
      "2025-10-27 12:47:44,278 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0037, Critic loss=8007.1626, Entropy=0.8855\n",
      "2025-10-27 12:47:44,302 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0050, Critic loss=8000.3560, Entropy=0.8850\n",
      "2025-10-27 12:47:44,327 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0064, Critic loss=7994.0747, Entropy=0.8846\n",
      "2025-10-27 12:47:44,327 - PPOAgent - INFO - Training completed: Actor loss=-0.0064, Critic loss=7994.0747\n",
      "Starting episode: 5\n",
      "total_reward:  -118.76886 \n",
      "drop_improvement:  -8.79482 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -109.97404 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:47:56,464 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0000, Critic loss=7230.3486, Entropy=0.8836\n",
      "2025-10-27 12:47:56,489 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0007, Critic loss=7224.9580, Entropy=0.8832\n",
      "2025-10-27 12:47:56,514 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0017, Critic loss=7219.8193, Entropy=0.8827\n",
      "2025-10-27 12:47:56,538 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0029, Critic loss=7214.9551, Entropy=0.8822\n",
      "2025-10-27 12:47:56,563 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0042, Critic loss=7210.3721, Entropy=0.8816\n",
      "2025-10-27 12:47:56,588 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0057, Critic loss=7206.0615, Entropy=0.8808\n",
      "2025-10-27 12:47:56,612 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0073, Critic loss=7202.0127, Entropy=0.8800\n",
      "2025-10-27 12:47:56,637 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0090, Critic loss=7198.1855, Entropy=0.8791\n",
      "2025-10-27 12:47:56,637 - PPOAgent - INFO - Training completed: Actor loss=-0.0090, Critic loss=7198.1855\n",
      "Starting episode: 6\n",
      "total_reward:  -58.30091 \n",
      "drop_improvement:  8.61219 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -66.91310 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:48:08,893 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=5234.1763, Entropy=0.8768\n",
      "2025-10-27 12:48:08,921 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0006, Critic loss=5231.2490, Entropy=0.8755\n",
      "2025-10-27 12:48:08,949 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0015, Critic loss=5228.3203, Entropy=0.8741\n",
      "2025-10-27 12:48:08,977 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0027, Critic loss=5225.4106, Entropy=0.8726\n",
      "2025-10-27 12:48:09,005 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0041, Critic loss=5222.5513, Entropy=0.8710\n",
      "2025-10-27 12:48:09,031 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0056, Critic loss=5219.7666, Entropy=0.8696\n",
      "2025-10-27 12:48:09,056 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0073, Critic loss=5217.0693, Entropy=0.8680\n",
      "2025-10-27 12:48:09,080 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0090, Critic loss=5214.4551, Entropy=0.8665\n",
      "2025-10-27 12:48:09,081 - PPOAgent - INFO - Training completed: Actor loss=-0.0090, Critic loss=5214.4551\n",
      "Starting episode: 7\n",
      "total_reward:  -10.41434 \n",
      "drop_improvement:  9.41646 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -19.83080 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:48:22,990 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0000, Critic loss=2810.5649, Entropy=0.8657\n",
      "2025-10-27 12:48:23,016 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0011, Critic loss=2808.8770, Entropy=0.8641\n",
      "2025-10-27 12:48:23,040 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0025, Critic loss=2807.1665, Entropy=0.8625\n",
      "2025-10-27 12:48:23,065 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0041, Critic loss=2805.4280, Entropy=0.8609\n",
      "2025-10-27 12:48:23,089 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0060, Critic loss=2803.6677, Entropy=0.8593\n",
      "2025-10-27 12:48:23,115 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0080, Critic loss=2801.8948, Entropy=0.8575\n",
      "2025-10-27 12:48:23,140 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0101, Critic loss=2800.1213, Entropy=0.8556\n",
      "2025-10-27 12:48:23,167 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0123, Critic loss=2798.3535, Entropy=0.8536\n",
      "2025-10-27 12:48:23,167 - PPOAgent - INFO - Training completed: Actor loss=-0.0123, Critic loss=2798.3535\n",
      "Starting episode: 8\n",
      "total_reward:  -61.15771 \n",
      "drop_improvement:  -7.30975 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.01579 \n",
      "prb_improvement:  0.01579 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -56.37954 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:48:37,045 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=4126.6108, Entropy=0.8514\n",
      "2025-10-27 12:48:37,069 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0002, Critic loss=4124.2324, Entropy=0.8496\n",
      "2025-10-27 12:48:37,106 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0009, Critic loss=4121.8110, Entropy=0.8480\n",
      "2025-10-27 12:48:37,145 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0019, Critic loss=4119.3994, Entropy=0.8466\n",
      "2025-10-27 12:48:37,182 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0033, Critic loss=4117.0327, Entropy=0.8454\n",
      "2025-10-27 12:48:37,227 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0049, Critic loss=4114.7319, Entropy=0.8443\n",
      "2025-10-27 12:48:37,269 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0066, Critic loss=4112.4961, Entropy=0.8434\n",
      "2025-10-27 12:48:37,309 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0085, Critic loss=4110.3086, Entropy=0.8427\n",
      "2025-10-27 12:48:37,310 - PPOAgent - INFO - Training completed: Actor loss=-0.0085, Critic loss=4110.3086\n",
      "Starting episode: 9\n",
      "total_reward:  -71.24596 \n",
      "drop_improvement:  -2.89528 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00263 \n",
      "prb_improvement:  0.00263 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -70.85594 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:48:50,630 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0000, Critic loss=5030.1411, Entropy=0.8425\n",
      "2025-10-27 12:48:50,666 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0006, Critic loss=5027.4775, Entropy=0.8418\n",
      "2025-10-27 12:48:50,707 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0014, Critic loss=5024.7432, Entropy=0.8411\n",
      "2025-10-27 12:48:50,742 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0024, Critic loss=5021.9658, Entropy=0.8403\n",
      "2025-10-27 12:48:50,780 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0036, Critic loss=5019.1768, Entropy=0.8396\n",
      "2025-10-27 12:48:50,820 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0050, Critic loss=5016.4053, Entropy=0.8388\n",
      "2025-10-27 12:48:50,857 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0065, Critic loss=5013.6655, Entropy=0.8380\n",
      "2025-10-27 12:48:50,894 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0082, Critic loss=5010.9609, Entropy=0.8372\n",
      "2025-10-27 12:48:50,894 - PPOAgent - INFO - Training completed: Actor loss=-0.0082, Critic loss=5010.9609\n",
      "Starting episode: 10\n",
      "total_reward:  -52.94995 \n",
      "drop_improvement:  2.99047 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  -0.01842 \n",
      "prb_improvement:  -0.01842 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -55.90358 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:49:03,881 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=4938.5776, Entropy=0.8363\n",
      "2025-10-27 12:49:03,905 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0006, Critic loss=4936.5015, Entropy=0.8353\n",
      "2025-10-27 12:49:03,933 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0017, Critic loss=4934.3643, Entropy=0.8343\n",
      "2025-10-27 12:49:03,964 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0031, Critic loss=4932.1963, Entropy=0.8333\n",
      "2025-10-27 12:49:03,993 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0048, Critic loss=4930.0352, Entropy=0.8323\n",
      "2025-10-27 12:49:04,021 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0068, Critic loss=4927.9087, Entropy=0.8312\n",
      "2025-10-27 12:49:04,050 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0089, Critic loss=4925.8281, Entropy=0.8300\n",
      "2025-10-27 12:49:04,079 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0111, Critic loss=4923.7881, Entropy=0.8288\n",
      "2025-10-27 12:49:04,079 - PPOAgent - INFO - Training completed: Actor loss=-0.0111, Critic loss=4923.7881\n",
      "Starting episode: 11\n",
      "total_reward:  2.50000 \n",
      "drop_improvement:  10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -10.00000 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:49:17,336 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=12560.7812, Entropy=0.8270\n",
      "2025-10-27 12:49:17,361 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0001, Critic loss=12557.7549, Entropy=0.8252\n",
      "2025-10-27 12:49:17,386 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0005, Critic loss=12554.5762, Entropy=0.8232\n",
      "2025-10-27 12:49:17,410 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0014, Critic loss=12551.3086, Entropy=0.8210\n",
      "2025-10-27 12:49:17,435 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0025, Critic loss=12548.0410, Entropy=0.8186\n",
      "2025-10-27 12:49:17,460 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0039, Critic loss=12544.8398, Entropy=0.8161\n",
      "2025-10-27 12:49:17,485 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0056, Critic loss=12541.7373, Entropy=0.8135\n",
      "2025-10-27 12:49:17,509 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0074, Critic loss=12538.7275, Entropy=0.8108\n",
      "2025-10-27 12:49:17,509 - PPOAgent - INFO - Training completed: Actor loss=-0.0074, Critic loss=12538.7275\n",
      "Starting episode: 12\n",
      "total_reward:  -65.79504 \n",
      "drop_improvement:  -10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -55.79504 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:49:32,730 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=1277.0173, Entropy=0.8090\n",
      "2025-10-27 12:49:32,755 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0008, Critic loss=1275.9692, Entropy=0.8071\n",
      "2025-10-27 12:49:32,779 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0018, Critic loss=1274.8978, Entropy=0.8057\n",
      "2025-10-27 12:49:32,803 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0030, Critic loss=1273.8051, Entropy=0.8048\n",
      "2025-10-27 12:49:32,828 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0045, Critic loss=1272.6997, Entropy=0.8043\n",
      "2025-10-27 12:49:32,852 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0061, Critic loss=1271.5929, Entropy=0.8042\n",
      "2025-10-27 12:49:32,876 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0079, Critic loss=1270.4934, Entropy=0.8042\n",
      "2025-10-27 12:49:32,901 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0099, Critic loss=1269.4060, Entropy=0.8045\n",
      "2025-10-27 12:49:32,901 - PPOAgent - INFO - Training completed: Actor loss=-0.0099, Critic loss=1269.4060\n",
      "Starting episode: 13\n",
      "total_reward:  -61.38975 \n",
      "drop_improvement:  -1.37890 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.10397 \n",
      "prb_improvement:  0.07474 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -62.68955 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:49:47,158 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=4636.5859, Entropy=0.8047\n",
      "2025-10-27 12:49:47,183 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0006, Critic loss=4633.9561, Entropy=0.8050\n",
      "2025-10-27 12:49:47,209 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0016, Critic loss=4631.3076, Entropy=0.8053\n",
      "2025-10-27 12:49:47,233 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0028, Critic loss=4628.6602, Entropy=0.8055\n",
      "2025-10-27 12:49:47,258 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0042, Critic loss=4626.0269, Entropy=0.8057\n",
      "2025-10-27 12:49:47,282 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0059, Critic loss=4623.4150, Entropy=0.8059\n",
      "2025-10-27 12:49:47,307 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0077, Critic loss=4620.8237, Entropy=0.8060\n",
      "2025-10-27 12:49:47,331 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0096, Critic loss=4618.2461, Entropy=0.8061\n",
      "2025-10-27 12:49:47,332 - PPOAgent - INFO - Training completed: Actor loss=-0.0096, Critic loss=4618.2461\n",
      "Starting episode: 14\n",
      "total_reward:  -83.81014 \n",
      "drop_improvement:  -3.94132 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.02734 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -82.39616 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:50:04,516 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=7635.5146, Entropy=0.8061\n",
      "2025-10-27 12:50:04,542 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0005, Critic loss=7632.4189, Entropy=0.8057\n",
      "2025-10-27 12:50:04,566 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0013, Critic loss=7629.3008, Entropy=0.8052\n",
      "2025-10-27 12:50:04,592 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0024, Critic loss=7626.1709, Entropy=0.8044\n",
      "2025-10-27 12:50:04,619 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0037, Critic loss=7623.0381, Entropy=0.8034\n",
      "2025-10-27 12:50:04,645 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0053, Critic loss=7619.9111, Entropy=0.8021\n",
      "2025-10-27 12:50:04,670 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0072, Critic loss=7616.7969, Entropy=0.8007\n",
      "2025-10-27 12:50:04,695 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0092, Critic loss=7613.6943, Entropy=0.7990\n",
      "2025-10-27 12:50:04,695 - PPOAgent - INFO - Training completed: Actor loss=-0.0092, Critic loss=7613.6943\n",
      "Starting episode: 15\n",
      "total_reward:  2.38375 \n",
      "drop_improvement:  10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  -0.05734 \n",
      "prb_improvement:  -0.05891 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -10.00000 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:50:18,790 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=3918.4194, Entropy=0.7971\n",
      "2025-10-27 12:50:18,814 - PPOAgent - INFO - Epoch 2/8: Actor loss=0.0000, Critic loss=3916.6230, Entropy=0.7953\n",
      "2025-10-27 12:50:18,839 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0003, Critic loss=3914.7754, Entropy=0.7934\n",
      "2025-10-27 12:50:18,863 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0009, Critic loss=3912.9143, Entropy=0.7916\n",
      "2025-10-27 12:50:18,888 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0017, Critic loss=3911.0732, Entropy=0.7899\n",
      "2025-10-27 12:50:18,913 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0028, Critic loss=3909.2715, Entropy=0.7882\n",
      "2025-10-27 12:50:18,937 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0041, Critic loss=3907.5078, Entropy=0.7865\n",
      "2025-10-27 12:50:18,961 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0055, Critic loss=3905.7710, Entropy=0.7848\n",
      "2025-10-27 12:50:18,962 - PPOAgent - INFO - Training completed: Actor loss=-0.0055, Critic loss=3905.7710\n",
      "Starting episode: 16\n",
      "total_reward:  -40.09116 \n",
      "drop_improvement:  -5.61352 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  -0.02368 \n",
      "prb_improvement:  -0.02842 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -36.92553 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:50:33,820 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=2757.2334, Entropy=0.7828\n",
      "2025-10-27 12:50:33,845 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0004, Critic loss=2755.4690, Entropy=0.7809\n",
      "2025-10-27 12:50:33,871 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0012, Critic loss=2753.6531, Entropy=0.7788\n",
      "2025-10-27 12:50:33,896 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0024, Critic loss=2751.8120, Entropy=0.7766\n",
      "2025-10-27 12:50:33,921 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0039, Critic loss=2749.9761, Entropy=0.7742\n",
      "2025-10-27 12:50:33,945 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0056, Critic loss=2748.1636, Entropy=0.7717\n",
      "2025-10-27 12:50:33,970 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0075, Critic loss=2746.3809, Entropy=0.7691\n",
      "2025-10-27 12:50:33,995 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0097, Critic loss=2744.6196, Entropy=0.7665\n",
      "2025-10-27 12:50:33,996 - PPOAgent - INFO - Training completed: Actor loss=-0.0097, Critic loss=2744.6196\n",
      "Starting episode: 17\n",
      "total_reward:  1.51023 \n",
      "drop_improvement:  9.00344 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00679 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -10.00000 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:50:46,860 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=1078.1637, Entropy=0.7641\n",
      "2025-10-27 12:50:46,885 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0008, Critic loss=1077.1240, Entropy=0.7615\n",
      "2025-10-27 12:50:46,910 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0019, Critic loss=1076.0579, Entropy=0.7589\n",
      "2025-10-27 12:50:46,933 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0032, Critic loss=1074.9736, Entropy=0.7564\n",
      "2025-10-27 12:50:46,958 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0048, Critic loss=1073.8851, Entropy=0.7540\n",
      "2025-10-27 12:50:46,983 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0066, Critic loss=1072.8043, Entropy=0.7518\n",
      "2025-10-27 12:50:47,008 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0085, Critic loss=1071.7380, Entropy=0.7497\n",
      "2025-10-27 12:50:47,033 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0106, Critic loss=1070.6858, Entropy=0.7477\n",
      "2025-10-27 12:50:47,033 - PPOAgent - INFO - Training completed: Actor loss=-0.0106, Critic loss=1070.6858\n",
      "Starting episode: 18\n",
      "total_reward:  -39.03173 \n",
      "drop_improvement:  -8.27174 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.01016 \n",
      "prb_improvement:  -0.00316 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -33.26700 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:51:02,077 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=3840.9873, Entropy=0.7454\n",
      "2025-10-27 12:51:02,120 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0013, Critic loss=3838.9014, Entropy=0.7433\n",
      "2025-10-27 12:51:02,168 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0029, Critic loss=3836.7607, Entropy=0.7409\n",
      "2025-10-27 12:51:02,216 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0047, Critic loss=3834.5908, Entropy=0.7384\n",
      "2025-10-27 12:51:02,262 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0068, Critic loss=3832.4214, Entropy=0.7356\n",
      "2025-10-27 12:51:02,307 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0091, Critic loss=3830.2744, Entropy=0.7326\n",
      "2025-10-27 12:51:02,348 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0117, Critic loss=3828.1582, Entropy=0.7294\n",
      "2025-10-27 12:51:02,386 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0144, Critic loss=3826.0676, Entropy=0.7261\n",
      "2025-10-27 12:51:02,387 - PPOAgent - INFO - Training completed: Actor loss=-0.0144, Critic loss=3826.0676\n",
      "Starting episode: 19\n",
      "total_reward:  -15.88311 \n",
      "drop_improvement:  2.90558 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  -0.03801 \n",
      "prb_improvement:  -0.01158 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -18.73910 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-27 12:51:16,111 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=3982.3284, Entropy=0.7228\n",
      "2025-10-27 12:51:16,136 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0008, Critic loss=3979.9937, Entropy=0.7196\n",
      "2025-10-27 12:51:16,161 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0019, Critic loss=3977.5713, Entropy=0.7165\n",
      "2025-10-27 12:51:16,185 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0033, Critic loss=3975.1067, Entropy=0.7137\n",
      "2025-10-27 12:51:16,210 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0050, Critic loss=3972.6497, Entropy=0.7110\n",
      "2025-10-27 12:51:16,234 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0070, Critic loss=3970.2351, Entropy=0.7085\n",
      "2025-10-27 12:51:16,258 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0092, Critic loss=3967.8743, Entropy=0.7062\n",
      "2025-10-27 12:51:16,283 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0115, Critic loss=3965.5581, Entropy=0.7040\n",
      "2025-10-27 12:51:16,283 - PPOAgent - INFO - Training completed: Actor loss=-0.0115, Critic loss=3965.5581\n",
      "Starting episode: 20\n",
      "total_reward:  -2.44505 \n",
      "drop_improvement:  5.01916 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.02105 \n",
      "prb_improvement:  0.01474 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -10.00000 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:51:29,632 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0000, Critic loss=328.9120, Entropy=0.7020\n",
      "2025-10-27 12:51:29,656 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0005, Critic loss=328.4146, Entropy=0.7000\n",
      "2025-10-27 12:51:29,680 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0014, Critic loss=327.8980, Entropy=0.6983\n",
      "2025-10-27 12:51:29,704 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0026, Critic loss=327.3689, Entropy=0.6966\n",
      "2025-10-27 12:51:29,729 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0040, Critic loss=326.8378, Entropy=0.6950\n",
      "2025-10-27 12:51:29,753 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0057, Critic loss=326.3129, Entropy=0.6935\n",
      "2025-10-27 12:51:29,778 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0076, Critic loss=325.7980, Entropy=0.6921\n",
      "2025-10-27 12:51:29,802 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0096, Critic loss=325.2927, Entropy=0.6908\n",
      "2025-10-27 12:51:29,803 - PPOAgent - INFO - Training completed: Actor loss=-0.0096, Critic loss=325.2927\n",
      "Starting episode: 21\n",
      "total_reward:  -61.77704 \n",
      "drop_improvement:  -10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.04000 \n",
      "prb_improvement:  -0.00316 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -54.31388 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:51:41,465 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=1042.3649, Entropy=0.6897\n",
      "2025-10-27 12:51:41,489 - PPOAgent - INFO - Epoch 2/8: Actor loss=0.0001, Critic loss=1041.4719, Entropy=0.6881\n",
      "2025-10-27 12:51:41,514 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0003, Critic loss=1040.5510, Entropy=0.6864\n",
      "2025-10-27 12:51:41,538 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0011, Critic loss=1039.6135, Entropy=0.6845\n",
      "2025-10-27 12:51:41,562 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0023, Critic loss=1038.6743, Entropy=0.6824\n",
      "2025-10-27 12:51:41,587 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0037, Critic loss=1037.7454, Entropy=0.6803\n",
      "2025-10-27 12:51:41,611 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0055, Critic loss=1036.8318, Entropy=0.6780\n",
      "2025-10-27 12:51:41,635 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0075, Critic loss=1035.9321, Entropy=0.6756\n",
      "2025-10-27 12:51:41,636 - PPOAgent - INFO - Training completed: Actor loss=-0.0075, Critic loss=1035.9321\n",
      "Starting episode: 22\n",
      "total_reward:  -14.88282 \n",
      "drop_improvement:  6.14260 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.03758 \n",
      "prb_improvement:  0.03789 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -23.60089 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:51:55,058 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=3840.1257, Entropy=0.6724\n",
      "2025-10-27 12:51:55,097 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0012, Critic loss=3838.0930, Entropy=0.6701\n",
      "2025-10-27 12:51:55,137 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0026, Critic loss=3836.0364, Entropy=0.6680\n",
      "2025-10-27 12:51:55,182 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0043, Critic loss=3833.9746, Entropy=0.6659\n",
      "2025-10-27 12:51:55,228 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0061, Critic loss=3831.9231, Entropy=0.6640\n",
      "2025-10-27 12:51:55,271 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0082, Critic loss=3829.8867, Entropy=0.6620\n",
      "2025-10-27 12:51:55,307 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0103, Critic loss=3827.8635, Entropy=0.6602\n",
      "2025-10-27 12:51:55,343 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0127, Critic loss=3825.8445, Entropy=0.6583\n",
      "2025-10-27 12:51:55,344 - PPOAgent - INFO - Training completed: Actor loss=-0.0127, Critic loss=3825.8445\n",
      "Starting episode: 23\n",
      "total_reward:  -47.63906 \n",
      "drop_improvement:  -4.43133 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  -0.00390 \n",
      "prb_improvement:  0.05368 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -45.75752 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:52:11,261 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=5203.4956, Entropy=0.6564\n",
      "2025-10-27 12:52:11,286 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0009, Critic loss=5200.3975, Entropy=0.6552\n",
      "2025-10-27 12:52:11,311 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0024, Critic loss=5197.2598, Entropy=0.6545\n",
      "2025-10-27 12:52:11,335 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0043, Critic loss=5194.1289, Entropy=0.6545\n",
      "2025-10-27 12:52:11,361 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0068, Critic loss=5191.0337, Entropy=0.6550\n",
      "2025-10-27 12:52:11,385 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0096, Critic loss=5187.9854, Entropy=0.6561\n",
      "2025-10-27 12:52:11,410 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0128, Critic loss=5184.9717, Entropy=0.6577\n",
      "2025-10-27 12:52:11,435 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0162, Critic loss=5181.9697, Entropy=0.6599\n",
      "2025-10-27 12:52:11,435 - PPOAgent - INFO - Training completed: Actor loss=-0.0162, Critic loss=5181.9697\n",
      "Starting episode: 24\n",
      "total_reward:  2.41649 \n",
      "drop_improvement:  10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  -0.02983 \n",
      "prb_improvement:  -0.05368 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -10.00000 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:52:24,264 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=232.7159, Entropy=0.6632\n",
      "2025-10-27 12:52:24,300 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0007, Critic loss=232.3821, Entropy=0.6647\n",
      "2025-10-27 12:52:24,328 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0023, Critic loss=232.0379, Entropy=0.6649\n",
      "2025-10-27 12:52:24,353 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0045, Critic loss=231.6964, Entropy=0.6640\n",
      "2025-10-27 12:52:24,378 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0074, Critic loss=231.3664, Entropy=0.6620\n",
      "2025-10-27 12:52:24,402 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0109, Critic loss=231.0502, Entropy=0.6593\n",
      "2025-10-27 12:52:24,427 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0149, Critic loss=230.7437, Entropy=0.6560\n",
      "2025-10-27 12:52:24,452 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0192, Critic loss=230.4400, Entropy=0.6522\n",
      "2025-10-27 12:52:24,453 - PPOAgent - INFO - Training completed: Actor loss=-0.0192, Critic loss=230.4400\n",
      "Starting episode: 25\n",
      "total_reward:  -52.05411 \n",
      "drop_improvement:  -10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00529 \n",
      "prb_improvement:  0.00316 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -44.56256 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:52:37,466 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=5085.5405, Entropy=0.6479\n",
      "2025-10-27 12:52:37,494 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0006, Critic loss=5082.5557, Entropy=0.6443\n",
      "2025-10-27 12:52:37,524 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0015, Critic loss=5079.4282, Entropy=0.6411\n",
      "2025-10-27 12:52:37,548 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0027, Critic loss=5076.2915, Entropy=0.6383\n",
      "2025-10-27 12:52:37,573 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0042, Critic loss=5073.2471, Entropy=0.6358\n",
      "2025-10-27 12:52:37,599 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0059, Critic loss=5070.3271, Entropy=0.6334\n",
      "2025-10-27 12:52:37,625 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0079, Critic loss=5067.4995, Entropy=0.6313\n",
      "2025-10-27 12:52:37,654 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0100, Critic loss=5064.6973, Entropy=0.6293\n",
      "2025-10-27 12:52:37,655 - PPOAgent - INFO - Training completed: Actor loss=-0.0100, Critic loss=5064.6973\n",
      "Starting episode: 26\n",
      "total_reward:  -17.52178 \n",
      "drop_improvement:  4.09100 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00111 \n",
      "prb_improvement:  -0.00632 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -24.10757 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-27 12:52:55,198 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=236.6761, Entropy=0.6277\n",
      "2025-10-27 12:52:55,222 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0003, Critic loss=236.1082, Entropy=0.6256\n",
      "2025-10-27 12:52:55,247 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0009, Critic loss=235.5274, Entropy=0.6234\n",
      "2025-10-27 12:52:55,271 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0017, Critic loss=234.9460, Entropy=0.6210\n",
      "2025-10-27 12:52:55,296 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0029, Critic loss=234.3736, Entropy=0.6185\n",
      "2025-10-27 12:52:55,320 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0043, Critic loss=233.8130, Entropy=0.6159\n",
      "2025-10-27 12:52:55,345 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0059, Critic loss=233.2605, Entropy=0.6133\n",
      "2025-10-27 12:52:55,371 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0077, Critic loss=232.7088, Entropy=0.6105\n",
      "2025-10-27 12:52:55,372 - PPOAgent - INFO - Training completed: Actor loss=-0.0077, Critic loss=232.7088\n",
      "Starting episode: 27\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 253, in <module>\n",
      "    main(n_parallel_envs=args.n_envs, scenarios_dir=args.scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 67, in main\n",
      "    results = run_scenario_with_rl_agent_parallel(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 182, in run_scenario_with_rl_agent_parallel\n",
      "    next_obs, rewards, new_dones, infos = envs.step(actions)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simple_parallel_env.py\", line 127, in step\n",
      "    results = [conn.recv() for conn in self.parent_conns]\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simple_parallel_env.py\", line 127, in <listcomp>\n",
      "    results = [conn.recv() for conn in self.parent_conns]\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('app/energy_agent/config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    num_envs = config['n_envs']\n",
    "\n",
    "!python3 app/main_run_scenarios_parallel.py --n-envs {num_envs} --scenarios-dir app/train_scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/energy_agent/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: False\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model_cur.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "qos_grad_coeff: 10.0\n",
    "energy_grad_coeff: 5.0\n",
    "cpu_grad_coeff: 1.0\n",
    "prb_grad_coeff: 1.0\n",
    "stability_penalty: 2.0\n",
    "energy_consumption_penalty: 1000.0\n",
    "\n",
    "entropy_coef: 0.01\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 2\n",
    "buffer_size: 4\n",
    "n_envs: 4\n",
    "hidden_dim: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/anaconda3/envs/mira/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "\n",
      "=== Running Benchmark Suite (3 scenarios) ===\n",
      "\n",
      "\n",
      "--- Scenario 1/3: dense_urban ---\n",
      "Loaded scenario: Dense Urban\n",
      "Error in scenario dense_urban: 'energy_coeff'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/main_run_scenarios_python.py\", line 61, in main\n",
      "    results = run_scenario_with_rl_agent(scenario=name, seed=seed, scenarios_dir=scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/main_run_scenarios_python.py\", line 113, in run_scenario_with_rl_agent\n",
      "    agent = RLAgent(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/energy_agent/rl_agent.py\", line 71, in __init__\n",
      "    self.energy_coeff = config['energy_coeff']\n",
      "KeyError: 'energy_coeff'\n",
      "\n",
      "--- Scenario 2/3: rural ---\n",
      "Loaded scenario: 3GPP Rural Macro\n",
      "Error in scenario rural: 'energy_coeff'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/main_run_scenarios_python.py\", line 61, in main\n",
      "    results = run_scenario_with_rl_agent(scenario=name, seed=seed, scenarios_dir=scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/main_run_scenarios_python.py\", line 113, in run_scenario_with_rl_agent\n",
      "    agent = RLAgent(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/energy_agent/rl_agent.py\", line 71, in __init__\n",
      "    self.energy_coeff = config['energy_coeff']\n",
      "KeyError: 'energy_coeff'\n",
      "\n",
      "--- Scenario 3/3: urban_macro ---\n",
      "Loaded scenario: 3GPP Urban Macro\n",
      "Error in scenario urban_macro: 'energy_coeff'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/main_run_scenarios_python.py\", line 61, in main\n",
      "    results = run_scenario_with_rl_agent(scenario=name, seed=seed, scenarios_dir=scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/main_run_scenarios_python.py\", line 113, in run_scenario_with_rl_agent\n",
      "    agent = RLAgent(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving/app/energy_agent/rl_agent.py\", line 71, in __init__\n",
      "    self.energy_coeff = config['energy_coeff']\n",
      "KeyError: 'energy_coeff'\n",
      "\n",
      "Written 3 energy values to energies.txt\n",
      "\n",
      "energies.txt generated with 3 values\n",
      "\n",
      "Energy values written to energies.txt:\n",
      "  Scenario 1 (dense_urban): 0.000000 kWh\n",
      "  Scenario 2 (rural): 0.000000 kWh\n",
      "  Scenario 3 (urban_macro): 0.000000 kWh\n"
     ]
    }
   ],
   "source": [
    "!python3 app/main_run_scenarios_python.py --scenarios-dir app/test_scenarios"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
