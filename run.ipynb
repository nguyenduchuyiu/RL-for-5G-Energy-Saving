{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nguyenduchuyiu/RL-for-5G-Energy-Saving.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd RL-for-5G-Energy-Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/energy_agent/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: True\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model_last.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "energy_coeff: 0.1\n",
    "drop_magnitude_penalty_coef: 1.0\n",
    "latency_magnitude_penalty_coef: 1.0\n",
    "cpu_magnitude_penalty_coef: 1.0\n",
    "prb_magnitude_penalty_coef: 1.0\n",
    "improvement_coeff: 5.0\n",
    "\n",
    "violation_event_penalty: -5.0\n",
    "energy_consumption_penalty: -1000.0\n",
    "\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 2\n",
    "buffer_size: 4\n",
    "n_envs: 4\n",
    "hidden_dim: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/dense_urban.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/dense_urban.json\n",
    "{\n",
    "  \"name\": \"Dense Urban\",\n",
    "  \"description\": \"3GPP Dense urban with macro and micro cells\",\n",
    "  \"deploymentScenario\": \"dense_urban\",\n",
    "  \n",
    "  \"carrierFrequency\": 4e9,\n",
    "  \"systemBandwidth\": 200e6,\n",
    "  \"layout\": \"two_layer\",\n",
    "  \"isd\": 200,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"Uniform/macro\",\n",
    "  \"ueSpeed\": 3,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 10,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 6144,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 20,\n",
    "  \"peakHourMultiplier\": 1.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/train_scenarios/urban_macro.json\n",
    "    {\n",
    "  \"name\": \"3GPP Urban Macro\",\n",
    "  \"description\": \"3GPP Urban macro deployment scenario with large cells and continuous coverage\",\n",
    "  \"deploymentScenario\": \"urban_macro\",\n",
    "  \n",
    "  \"carrierFrequency\": 2e9,\n",
    "  \"systemBandwidth\": 100e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 500,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 30,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 0,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 25,\n",
    "  \"peakHourMultiplier\": 1.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/train_scenarios/rural.json\n",
    "{\n",
    "  \"name\": \"3GPP Rural Macro\",\n",
    "  \"description\": \"3GPP Rural deployment scenario with wide area coverage for high speed vehicles\",\n",
    "  \"deploymentScenario\": \"rural\",\n",
    "  \n",
    "  \"carrierFrequency\": 700e6,\n",
    "  \"systemBandwidth\": 20e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 1732,\n",
    "  \n",
    "  \"numSites\": 19,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 35,\n",
    "  \"cellRadius\": 1000,\n",
    "  \n",
    "  \"numUEs\": 100,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 120,\n",
    "  \"indoorRatio\": 0.5,\n",
    "  \"outdoorSpeed\": 120,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1200,\n",
    "  \"idlePower\": 300,\n",
    "  \n",
    "  \"simTime\": 0,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -115,\n",
    "  \"rsrpTargetThreshold\": -105,\n",
    "  \"rsrpMeasurementThreshold\": -120,\n",
    "  \"dropCallThreshold\": 2,\n",
    "  \"latencyThreshold\": 100,\n",
    "  \"cpuThreshold\": 90,\n",
    "  \"prbThreshold\": 90,\n",
    "  \n",
    "  \"trafficLambda\": 10,\n",
    "  \"peakHourMultiplier\": 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/anaconda3/envs/mira/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "\n",
      "=== Running Benchmark Suite (3 scenarios) ===\n",
      "Scenarios directory: app/train_scenarios/\n",
      "Using 4 parallel environments per scenario\n",
      "Loaded scenarios: ['dense_urban', 'rural', 'urban_macro']\n",
      "\n",
      "\n",
      "--- Scenario 1/3: dense_urban ---\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "Created 4 parallel environments\n",
      "  Scenario: dense_urban\n",
      "  Action dim: 21\n",
      "  Obs dim: 280\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "2025-10-24 00:13:12,002 - PPOAgent - INFO - PPO Agent initialized: 21 cells, 300 UEs\n",
      "2025-10-24 00:13:12,002 - PPOAgent - INFO - State dim: 1444, Action dim: 100\n",
      "2025-10-24 00:13:12,002 - PPOAgent - INFO - Device: cuda\n",
      "2025-10-24 00:13:12,002 - PPOAgent - INFO - No checkpoint found at app/energy_agent/models/ppo_model_last.pth\n",
      "Created 7 sites for dense_urban scenarioCreated 7 sites for dense_urban scenario\n",
      "\n",
      "Configuring cells for dense_urban scenario...Configuring cells for dense_urban scenario...\n",
      "\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Created 7 sites for dense_urban scenario\n",
      "Configuring cells for dense_urban scenario...\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Created 7 sites for dense_urban scenario\n",
      "Configuring cells for dense_urban scenario...\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "===================Starting scenario===================\n",
      "===================Starting episode: 1===================\n",
      "Training with 4 parallel environments...\n",
      "Total steps: 6144\n",
      "Total reward: -40.724002838134766\n",
      "Energy efficiency reward: -15.997461318969727\n",
      "Drop penalty: -6.920202732086182\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -11.928568840026855\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -5.877770900726318\n",
      "Total reward: -24.400390625\n",
      "Energy efficiency reward: -18.515625\n",
      "Drop penalty: 0.0\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: 0.0\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -5.884765625\n",
      "Total reward: -31.579803466796875\n",
      "Energy efficiency reward: -11.79843807220459\n",
      "Drop penalty: -5.473757266998291\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -8.44150161743164\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -5.866106986999512\n",
      "Total reward: -52.7723388671875\n",
      "Energy efficiency reward: -27.73828125\n",
      "Drop penalty: -7.019035816192627\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -12.104639053344727\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -5.910384178161621\n",
      "=====================Ending episode: 1=====================\n",
      "2025-10-24 00:13:38,178 - PPOAgent - INFO - Episode =1,\n",
      "Episode Reward=-37.37,\n",
      "Drop Penalty=-4.85,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-8.12,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-18.51,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-5.88,\n",
      "\n",
      "2025-10-24 00:13:38,779 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0002, Critic loss=1476.8192\n",
      "2025-10-24 00:13:38,809 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0040, Critic loss=1378.9476\n",
      "2025-10-24 00:13:38,837 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0076, Critic loss=1311.3269\n",
      "2025-10-24 00:13:38,865 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0112, Critic loss=1268.4294\n",
      "2025-10-24 00:13:38,894 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0152, Critic loss=1236.9633\n",
      "2025-10-24 00:13:38,925 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0194, Critic loss=1211.2330\n",
      "2025-10-24 00:13:38,954 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0239, Critic loss=1193.6949\n",
      "2025-10-24 00:13:38,984 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0286, Critic loss=1179.8045\n",
      "2025-10-24 00:13:38,985 - PPOAgent - INFO - Training completed: Actor loss=-0.4203, Critic loss=839.5695\n",
      "===================Starting episode: 2===================\n",
      "Total reward: -478.6568603515625\n",
      "Energy efficiency reward: -451.9947204589844\n",
      "Drop penalty: -12.638492584228516\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -6.8903422355651855\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.133312225341797\n",
      "Total reward: -473.68072509765625\n",
      "Energy efficiency reward: -458.4691467285156\n",
      "Drop penalty: -5.0381178855896\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -3.01518177986145\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.158290863037109\n",
      "Total reward: -482.8794860839844\n",
      "Energy efficiency reward: -450.9056701660156\n",
      "Drop penalty: -13.618246078491211\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -11.236923217773438\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.118622779846191\n",
      "Total reward: -459.00506591796875\n",
      "Energy efficiency reward: -437.7300720214844\n",
      "Drop penalty: -10.034481048583984\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -4.114185333251953\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.1263017654418945\n",
      "=====================Ending episode: 2=====================\n",
      "2025-10-24 00:13:52,990 - PPOAgent - INFO - Episode =2,\n",
      "Episode Reward=-473.56,\n",
      "Drop Penalty=-10.33,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-6.31,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-449.77,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.13,\n",
      "\n",
      "2025-10-24 00:13:53,029 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0004, Critic loss=224235.1016\n",
      "2025-10-24 00:13:53,061 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0005, Critic loss=224058.9844\n",
      "2025-10-24 00:13:53,092 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0024, Critic loss=223926.1719\n",
      "2025-10-24 00:13:53,121 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0038, Critic loss=223823.2266\n",
      "2025-10-24 00:13:53,150 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0054, Critic loss=223734.3906\n",
      "2025-10-24 00:13:53,181 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0072, Critic loss=223652.0391\n",
      "2025-10-24 00:13:53,210 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0094, Critic loss=223574.8828\n",
      "2025-10-24 00:13:53,239 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0112, Critic loss=223507.3516\n",
      "2025-10-24 00:13:53,239 - PPOAgent - INFO - Training completed: Actor loss=-0.2738, Critic loss=221082.4219\n",
      "===================Starting episode: 3===================\n",
      "Total reward: -29.108036041259766\n",
      "Energy efficiency reward: -12.186914443969727\n",
      "Drop penalty: -11.161592483520508\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: 1.4076340198516846\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.167164325714111\n",
      "Total reward: -14.75349235534668\n",
      "Energy efficiency reward: -2.549999952316284\n",
      "Drop penalty: -5.0381178855896\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: 0.0\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.165374279022217\n",
      "Total reward: -19.18067169189453\n",
      "Energy efficiency reward: -8.022265434265137\n",
      "Drop penalty: -8.865493774414062\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: 4.847994804382324\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.140906810760498\n",
      "Total reward: -17.081039428710938\n",
      "Energy efficiency reward: -5.967187404632568\n",
      "Drop penalty: -7.416788101196289\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: 3.4458136558532715\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.142876625061035\n",
      "=====================Ending episode: 3=====================\n",
      "2025-10-24 00:14:10,611 - PPOAgent - INFO - Episode =3,\n",
      "Episode Reward=-20.03,\n",
      "Drop Penalty=-8.12,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=2.43,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-7.18,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.15,\n",
      "\n",
      "2025-10-24 00:14:10,645 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0002, Critic loss=427.8566\n",
      "2025-10-24 00:14:10,675 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0009, Critic loss=422.5596\n",
      "2025-10-24 00:14:10,702 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0022, Critic loss=419.3302\n",
      "2025-10-24 00:14:10,730 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0036, Critic loss=417.3563\n",
      "2025-10-24 00:14:10,760 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0059, Critic loss=415.3289\n",
      "2025-10-24 00:14:10,787 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0082, Critic loss=413.5927\n",
      "2025-10-24 00:14:10,817 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0105, Critic loss=411.7101\n",
      "2025-10-24 00:14:10,844 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0126, Critic loss=410.0019\n",
      "2025-10-24 00:14:10,844 - PPOAgent - INFO - Training completed: Actor loss=-0.3035, Critic loss=310.3845\n",
      "===================Starting episode: 4===================\n",
      "Total reward: 17.136852264404297\n",
      "Energy efficiency reward: 25.8330078125\n",
      "Drop penalty: -6.978704452514648\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: 5.377955436706543\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.0954060554504395\n",
      "Total reward: -6.768596172332764\n",
      "Energy efficiency reward: 10.21933650970459\n",
      "Drop penalty: -5.951049327850342\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -3.8998961448669434\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.136987209320068\n",
      "Total reward: -10.093310356140137\n",
      "Energy efficiency reward: -5.153515815734863\n",
      "Drop penalty: -5.2279253005981445\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: 7.443352699279785\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.155221939086914\n",
      "Total reward: 4.075957298278809\n",
      "Energy efficiency reward: 20.464649200439453\n",
      "Drop penalty: -8.171424865722656\n",
      "Latency penalty: 0.0\n",
      "CPU penalty: 0.0\n",
      "PRB penalty: 0.0\n",
      "Drop improvement: -1.1312365531921387\n",
      "Latency improvement: 0.0\n",
      "Energy consumption penalty: -7.086030006408691\n",
      "=====================Ending episode: 4=====================\n",
      "2025-10-24 00:14:25,261 - PPOAgent - INFO - Episode =4,\n",
      "Episode Reward=1.09,\n",
      "Drop Penalty=-6.58,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=1.95,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=12.84,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.12,\n",
      "\n",
      "2025-10-24 00:14:25,346 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0001, Critic loss=114.5821\n",
      "2025-10-24 00:14:25,411 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0012, Critic loss=114.3497\n",
      "2025-10-24 00:14:25,473 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0029, Critic loss=113.6761\n",
      "2025-10-24 00:14:25,546 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0048, Critic loss=112.7167\n",
      "2025-10-24 00:14:25,610 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0075, Critic loss=111.5502\n",
      "2025-10-24 00:14:25,679 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0104, Critic loss=108.1707\n",
      "2025-10-24 00:14:25,745 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0133, Critic loss=106.4544\n",
      "2025-10-24 00:14:25,819 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0169, Critic loss=103.6627\n",
      "2025-10-24 00:14:25,820 - PPOAgent - INFO - Training completed: Actor loss=-0.2266, Critic loss=178.1739\n",
      "===================Starting episode: 5===================\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 253, in <module>\n",
      "    main(n_parallel_envs=args.n_envs, scenarios_dir=args.scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 67, in main\n",
      "    results = run_scenario_with_rl_agent_parallel(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 182, in run_scenario_with_rl_agent_parallel\n",
      "    next_obs, rewards, new_dones, infos = envs.step(actions)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simple_parallel_env.py\", line 127, in step\n",
      "    results = [conn.recv() for conn in self.parent_conns]\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simple_parallel_env.py\", line 127, in <listcomp>\n",
      "    results = [conn.recv() for conn in self.parent_conns]\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('app/energy_agent/config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    num_envs = config['n_envs']\n",
    "\n",
    "!python3 app/main_run_scenarios_parallel.py --n-envs {num_envs} --scenarios-dir app/train_scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: False\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model_6k.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "energy_coeff: 0.01\n",
    "drop_magnitude_penalty_coef: 1.0\n",
    "latency_magnitude_penalty_coef: 0.5\n",
    "cpu_magnitude_penalty_coef: 1.0\n",
    "prb_magnitude_penalty_coef: 1.0\n",
    "improvement_coeff: 0.5\n",
    "\n",
    "violation_event_penalty: -10.0\n",
    "energy_consumption_penalty: -10.0\n",
    "\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 64\n",
    "buffer_size: 2048\n",
    "n_envs: 4\n",
    "hidden_dim: 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 app/main_run_scenarios_python.py --scenarios-dir app/test_scenarios"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
