{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/nguyenduchuyiu/RL-for-5G-Energy-Saving.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd RL-for-5G-Energy-Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/energy_agent/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: True\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model_last.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "energy_coeff: 0.1\n",
    "drop_magnitude_penalty_coef: 100.0\n",
    "latency_magnitude_penalty_coef: 100.0\n",
    "cpu_magnitude_penalty_coef: 100.0\n",
    "prb_magnitude_penalty_coef: 100.0\n",
    "improvement_coeff: 5.0\n",
    "\n",
    "violation_event_penalty: -5.0\n",
    "energy_consumption_penalty: -1000.0\n",
    "\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 2\n",
    "buffer_size: 4\n",
    "n_envs: 4\n",
    "hidden_dim: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/dense_urban.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/dense_urban.json\n",
    "{\n",
    "  \"name\": \"Dense Urban\",\n",
    "  \"description\": \"3GPP Dense urban with macro and micro cells\",\n",
    "  \"deploymentScenario\": \"dense_urban\",\n",
    "  \n",
    "  \"carrierFrequency\": 4e9,\n",
    "  \"systemBandwidth\": 200e6,\n",
    "  \"layout\": \"two_layer\",\n",
    "  \"isd\": 200,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"Uniform/macro\",\n",
    "  \"ueSpeed\": 3,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 10,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 6144,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 20,\n",
    "  \"peakHourMultiplier\": 1.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/train_scenarios/urban_macro.json\n",
    "    {\n",
    "  \"name\": \"3GPP Urban Macro\",\n",
    "  \"description\": \"3GPP Urban macro deployment scenario with large cells and continuous coverage\",\n",
    "  \"deploymentScenario\": \"urban_macro\",\n",
    "  \n",
    "  \"carrierFrequency\": 2e9,\n",
    "  \"systemBandwidth\": 100e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 500,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 30,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 0,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 25,\n",
    "  \"peakHourMultiplier\": 1.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/train_scenarios/rural.json\n",
    "{\n",
    "  \"name\": \"3GPP Rural Macro\",\n",
    "  \"description\": \"3GPP Rural deployment scenario with wide area coverage for high speed vehicles\",\n",
    "  \"deploymentScenario\": \"rural\",\n",
    "  \n",
    "  \"carrierFrequency\": 700e6,\n",
    "  \"systemBandwidth\": 20e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 1732,\n",
    "  \n",
    "  \"numSites\": 19,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 35,\n",
    "  \"cellRadius\": 1000,\n",
    "  \n",
    "  \"numUEs\": 100,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 120,\n",
    "  \"indoorRatio\": 0.5,\n",
    "  \"outdoorSpeed\": 120,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1200,\n",
    "  \"idlePower\": 300,\n",
    "  \n",
    "  \"simTime\": 0,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -115,\n",
    "  \"rsrpTargetThreshold\": -105,\n",
    "  \"rsrpMeasurementThreshold\": -120,\n",
    "  \"dropCallThreshold\": 2,\n",
    "  \"latencyThreshold\": 100,\n",
    "  \"cpuThreshold\": 90,\n",
    "  \"prbThreshold\": 90,\n",
    "  \n",
    "  \"trafficLambda\": 10,\n",
    "  \"peakHourMultiplier\": 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/anaconda3/envs/mira/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "\n",
      "=== Running Benchmark Suite (3 scenarios) ===\n",
      "Scenarios directory: app/train_scenarios/\n",
      "Using 4 parallel environments per scenario\n",
      "Loaded scenarios: ['dense_urban', 'rural', 'urban_macro']\n",
      "\n",
      "\n",
      "--- Scenario 1/3: dense_urban ---\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "Created 4 parallel environments\n",
      "  Scenario: dense_urban\n",
      "  Action dim: 21\n",
      "  Obs dim: 280\n",
      "Loaded scenario: Dense Urban\n",
      "Loaded scenario: Dense Urban\n",
      "2025-10-23 15:14:22,571 - PPOAgent - INFO - PPO Agent initialized: 21 cells, 300 UEs\n",
      "2025-10-23 15:14:22,571 - PPOAgent - INFO - State dim: 1440, Action dim: 100\n",
      "2025-10-23 15:14:22,571 - PPOAgent - INFO - Device: cuda\n",
      "2025-10-23 15:14:22,571 - PPOAgent - INFO - No checkpoint found at app/energy_agent/models/ppo_model_last.pth\n",
      "Created 7 sites for dense_urban scenarioCreated 7 sites for dense_urban scenario\n",
      "\n",
      "Configuring cells for dense_urban scenario...Configuring cells for dense_urban scenario...\n",
      "\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Created 7 sites for dense_urban scenarioCreated 7 sites for dense_urban scenario\n",
      "\n",
      "Configuring cells for dense_urban scenario...Configuring cells for dense_urban scenario...\n",
      "\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "===================Starting scenario===================\n",
      "===================Starting episode: 1===================\n",
      "Training with 4 parallel environments...\n",
      "Total steps: 6144\n",
      "=====================Ending episode: 1=====================\n",
      "2025-10-23 15:14:41,544 - PPOAgent - INFO - Episode =1,\n",
      "Episode Reward=-108.67,\n",
      "Drop Penalty=-73.01,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-6.99,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-22.76,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-5.90,\n",
      "\n",
      "2025-10-23 15:14:41,940 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0007, Critic loss=17475.3645\n",
      "2025-10-23 15:14:41,952 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0026, Critic loss=17226.2195\n",
      "2025-10-23 15:14:41,964 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0046, Critic loss=17079.2100\n",
      "2025-10-23 15:14:41,976 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0071, Critic loss=16956.6140\n",
      "2025-10-23 15:14:41,989 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0100, Critic loss=16845.9543\n",
      "2025-10-23 15:14:42,003 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0125, Critic loss=16780.9365\n",
      "2025-10-23 15:14:42,015 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0155, Critic loss=16720.9946\n",
      "2025-10-23 15:14:42,029 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0184, Critic loss=16659.1984\n",
      "2025-10-23 15:14:42,029 - PPOAgent - INFO - Training completed: Actor loss=0.6885, Critic loss=31232.7363\n",
      "===================Starting episode: 2===================\n",
      "=====================Ending episode: 2=====================\n",
      "2025-10-23 15:14:51,369 - PPOAgent - INFO - Episode =2,\n",
      "Episode Reward=-658.56,\n",
      "Drop Penalty=-196.25,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-2.88,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-452.27,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.15,\n",
      "\n",
      "2025-10-23 15:14:51,390 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0001, Critic loss=479444.6719\n",
      "2025-10-23 15:14:51,403 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0008, Critic loss=479215.6719\n",
      "2025-10-23 15:14:51,415 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0022, Critic loss=479044.5000\n",
      "2025-10-23 15:14:51,428 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0041, Critic loss=478920.3125\n",
      "2025-10-23 15:14:51,441 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0065, Critic loss=478791.1875\n",
      "2025-10-23 15:14:51,454 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0090, Critic loss=478688.4219\n",
      "2025-10-23 15:14:51,466 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0120, Critic loss=478599.0312\n",
      "2025-10-23 15:14:51,479 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0147, Critic loss=478517.5000\n",
      "2025-10-23 15:14:51,479 - PPOAgent - INFO - Training completed: Actor loss=0.5030, Critic loss=674072.5625\n",
      "===================Starting episode: 3===================\n",
      "=====================Ending episode: 3=====================\n",
      "2025-10-23 15:15:00,801 - PPOAgent - INFO - Episode =3,\n",
      "Episode Reward=-296.06,\n",
      "Drop Penalty=-294.32,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-2.26,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=7.65,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.13,\n",
      "\n",
      "2025-10-23 15:15:00,816 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0004, Critic loss=114808.2344\n",
      "2025-10-23 15:15:00,829 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0002, Critic loss=114750.7305\n",
      "2025-10-23 15:15:00,844 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0014, Critic loss=114699.7188\n",
      "2025-10-23 15:15:00,857 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0034, Critic loss=114663.2812\n",
      "2025-10-23 15:15:00,870 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0056, Critic loss=114636.4199\n",
      "2025-10-23 15:15:00,884 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0083, Critic loss=114613.5732\n",
      "2025-10-23 15:15:00,898 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0111, Critic loss=114584.4199\n",
      "2025-10-23 15:15:00,911 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0138, Critic loss=114554.7344\n",
      "2025-10-23 15:15:00,911 - PPOAgent - INFO - Training completed: Actor loss=-0.4066, Critic loss=52856.0938\n",
      "===================Starting episode: 4===================\n",
      "=====================Ending episode: 4=====================\n",
      "2025-10-23 15:15:10,199 - PPOAgent - INFO - Episode =4,\n",
      "Episode Reward=-230.15,\n",
      "Drop Penalty=-231.37,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=1.02,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=7.32,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.11,\n",
      "\n",
      "2025-10-23 15:15:10,214 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0001, Critic loss=73215.4980\n",
      "2025-10-23 15:15:10,227 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0012, Critic loss=73196.0469\n",
      "2025-10-23 15:15:10,238 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0028, Critic loss=73174.9121\n",
      "2025-10-23 15:15:10,252 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0052, Critic loss=73156.7168\n",
      "2025-10-23 15:15:10,264 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0077, Critic loss=73134.7061\n",
      "2025-10-23 15:15:10,277 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0107, Critic loss=73116.7471\n",
      "2025-10-23 15:15:10,290 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0138, Critic loss=73098.4443\n",
      "2025-10-23 15:15:10,306 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0167, Critic loss=73083.6855\n",
      "2025-10-23 15:15:10,306 - PPOAgent - INFO - Training completed: Actor loss=-0.5699, Critic loss=22630.9336\n",
      "===================Starting episode: 5===================\n",
      "=====================Ending episode: 5=====================\n",
      "2025-10-23 15:15:19,721 - PPOAgent - INFO - Episode =5,\n",
      "Episode Reward=-190.85,\n",
      "Drop Penalty=-182.59,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=0.96,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-2.10,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.12,\n",
      "\n",
      "2025-10-23 15:15:19,739 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0003, Critic loss=56707.4321\n",
      "2025-10-23 15:15:19,753 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0020, Critic loss=56691.9248\n",
      "2025-10-23 15:15:19,767 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0043, Critic loss=56673.4570\n",
      "2025-10-23 15:15:19,780 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0070, Critic loss=56658.2617\n",
      "2025-10-23 15:15:19,796 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0097, Critic loss=56642.6191\n",
      "2025-10-23 15:15:19,812 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0129, Critic loss=56631.1602\n",
      "2025-10-23 15:15:19,828 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0162, Critic loss=56615.4619\n",
      "2025-10-23 15:15:19,843 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0206, Critic loss=56597.2812\n",
      "2025-10-23 15:15:19,844 - PPOAgent - INFO - Training completed: Actor loss=0.5322, Critic loss=102397.9062\n",
      "===================Starting episode: 6===================\n",
      "=====================Ending episode: 6=====================\n",
      "2025-10-23 15:15:29,476 - PPOAgent - INFO - Episode =6,\n",
      "Episode Reward=-204.99,\n",
      "Drop Penalty=-206.67,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-0.43,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=9.20,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.09,\n",
      "\n",
      "2025-10-23 15:15:29,492 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=56308.3203\n",
      "2025-10-23 15:15:29,505 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0006, Critic loss=56289.8574\n",
      "2025-10-23 15:15:29,517 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0031, Critic loss=56275.2305\n",
      "2025-10-23 15:15:29,530 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0041, Critic loss=56258.6660\n",
      "2025-10-23 15:15:29,543 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0064, Critic loss=56242.3145\n",
      "2025-10-23 15:15:29,556 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0099, Critic loss=56225.9922\n",
      "2025-10-23 15:15:29,570 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0113, Critic loss=56209.5098\n",
      "2025-10-23 15:15:29,584 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0145, Critic loss=56194.8594\n",
      "2025-10-23 15:15:29,584 - PPOAgent - INFO - Training completed: Actor loss=-0.6834, Critic loss=19195.3594\n",
      "===================Starting episode: 7===================\n",
      "=====================Ending episode: 7=====================\n",
      "2025-10-23 15:15:39,181 - PPOAgent - INFO - Episode =7,\n",
      "Episode Reward=-42.65,\n",
      "Drop Penalty=-27.80,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=4.66,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-12.39,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.13,\n",
      "\n",
      "2025-10-23 15:15:39,196 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0002, Critic loss=2011.4242\n",
      "2025-10-23 15:15:39,208 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0017, Critic loss=2008.1671\n",
      "2025-10-23 15:15:39,222 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0039, Critic loss=2004.3578\n",
      "2025-10-23 15:15:39,235 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0063, Critic loss=2001.1617\n",
      "2025-10-23 15:15:39,247 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0092, Critic loss=1997.4424\n",
      "2025-10-23 15:15:39,259 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0123, Critic loss=1994.2755\n",
      "2025-10-23 15:15:39,272 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0155, Critic loss=1990.8085\n",
      "2025-10-23 15:15:39,284 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0190, Critic loss=1987.0723\n",
      "2025-10-23 15:15:39,284 - PPOAgent - INFO - Training completed: Actor loss=-0.0757, Critic loss=1903.4331\n",
      "===================Starting episode: 8===================\n",
      "=====================Ending episode: 8=====================\n",
      "2025-10-23 15:15:48,855 - PPOAgent - INFO - Episode =8,\n",
      "Episode Reward=-100.99,\n",
      "Drop Penalty=-88.04,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-0.93,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-4.87,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.14,\n",
      "\n",
      "2025-10-23 15:15:48,871 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0001, Critic loss=17022.0503\n",
      "2025-10-23 15:15:48,883 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0014, Critic loss=17014.4511\n",
      "2025-10-23 15:15:48,895 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0035, Critic loss=17003.7266\n",
      "2025-10-23 15:15:48,907 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0058, Critic loss=16994.5084\n",
      "2025-10-23 15:15:48,920 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0085, Critic loss=16989.2508\n",
      "2025-10-23 15:15:48,932 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0125, Critic loss=16979.1572\n",
      "2025-10-23 15:15:48,945 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0152, Critic loss=16969.3660\n",
      "2025-10-23 15:15:48,958 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0186, Critic loss=16963.1777\n",
      "2025-10-23 15:15:48,958 - PPOAgent - INFO - Training completed: Actor loss=-0.3658, Critic loss=8784.3281\n",
      "===================Starting episode: 9===================\n",
      "=====================Ending episode: 9=====================\n",
      "2025-10-23 15:15:58,536 - PPOAgent - INFO - Episode =9,\n",
      "Episode Reward=-100.11,\n",
      "Drop Penalty=-99.73,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-1.20,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=7.93,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.12,\n",
      "\n",
      "2025-10-23 15:15:58,551 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0001, Critic loss=19669.2125\n",
      "2025-10-23 15:15:58,563 - PPOAgent - INFO - Epoch 2/8: Actor loss=0.0001, Critic loss=19657.9990\n",
      "2025-10-23 15:15:58,575 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0016, Critic loss=19649.1504\n",
      "2025-10-23 15:15:58,588 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0037, Critic loss=19643.2773\n",
      "2025-10-23 15:15:58,601 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0055, Critic loss=19633.7815\n",
      "2025-10-23 15:15:58,612 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0082, Critic loss=19623.6592\n",
      "2025-10-23 15:15:58,624 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0103, Critic loss=19614.2926\n",
      "2025-10-23 15:15:58,637 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0131, Critic loss=19609.2560\n",
      "2025-10-23 15:15:58,637 - PPOAgent - INFO - Training completed: Actor loss=-0.7425, Critic loss=391.5628\n",
      "===================Starting episode: 10===================\n",
      "=====================Ending episode: 10=====================\n",
      "2025-10-23 15:16:08,762 - PPOAgent - INFO - Episode =10,\n",
      "Episode Reward=-76.01,\n",
      "Drop Penalty=-65.48,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=0.95,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-4.35,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.13,\n",
      "\n",
      "2025-10-23 15:16:08,777 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0001, Critic loss=9310.2670\n",
      "2025-10-23 15:16:08,790 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0013, Critic loss=9302.3240\n",
      "2025-10-23 15:16:08,805 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0034, Critic loss=9297.1063\n",
      "2025-10-23 15:16:08,817 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0064, Critic loss=9291.2439\n",
      "2025-10-23 15:16:08,828 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0092, Critic loss=9282.6262\n",
      "2025-10-23 15:16:08,841 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0132, Critic loss=9276.7451\n",
      "2025-10-23 15:16:08,854 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0163, Critic loss=9269.7617\n",
      "2025-10-23 15:16:08,865 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0207, Critic loss=9263.4735\n",
      "2025-10-23 15:16:08,865 - PPOAgent - INFO - Training completed: Actor loss=0.1905, Critic loss=14666.4277\n",
      "===================Starting episode: 11===================\n",
      "=====================Ending episode: 11=====================\n",
      "2025-10-23 15:16:19,048 - PPOAgent - INFO - Episode =11,\n",
      "Episode Reward=-77.56,\n",
      "Drop Penalty=-70.17,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-0.30,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=0.05,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.13,\n",
      "\n",
      "2025-10-23 15:16:19,064 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0000, Critic loss=10254.5811\n",
      "2025-10-23 15:16:19,076 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0009, Critic loss=10247.7230\n",
      "2025-10-23 15:16:19,088 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0028, Critic loss=10242.5888\n",
      "2025-10-23 15:16:19,102 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0060, Critic loss=10233.9666\n",
      "2025-10-23 15:16:19,116 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0087, Critic loss=10228.1536\n",
      "2025-10-23 15:16:19,128 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0133, Critic loss=10219.5001\n",
      "2025-10-23 15:16:19,139 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0175, Critic loss=10215.1059\n",
      "2025-10-23 15:16:19,153 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0218, Critic loss=10207.4861\n",
      "2025-10-23 15:16:19,153 - PPOAgent - INFO - Training completed: Actor loss=-0.3312, Critic loss=3954.2632\n",
      "===================Starting episode: 12===================\n",
      "=====================Ending episode: 12=====================\n",
      "2025-10-23 15:16:28,906 - PPOAgent - INFO - Episode =12,\n",
      "Episode Reward=-13.16,\n",
      "Drop Penalty=-14.97,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=2.76,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=6.17,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.11,\n",
      "\n",
      "2025-10-23 15:16:28,921 - PPOAgent - INFO - Epoch 1/8: Actor loss=-0.0001, Critic loss=626.0872\n",
      "2025-10-23 15:16:28,933 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0013, Critic loss=624.8974\n",
      "2025-10-23 15:16:28,945 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0033, Critic loss=623.7083\n",
      "2025-10-23 15:16:28,957 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0060, Critic loss=622.8708\n",
      "2025-10-23 15:16:28,971 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0087, Critic loss=621.3540\n",
      "2025-10-23 15:16:28,984 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0120, Critic loss=620.3781\n",
      "2025-10-23 15:16:28,997 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0153, Critic loss=619.5619\n",
      "2025-10-23 15:16:29,010 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0194, Critic loss=618.4940\n",
      "2025-10-23 15:16:29,010 - PPOAgent - INFO - Training completed: Actor loss=-0.2306, Critic loss=271.9246\n",
      "===================Starting episode: 13===================\n",
      "=====================Ending episode: 13=====================\n",
      "2025-10-23 15:16:38,756 - PPOAgent - INFO - Episode =13,\n",
      "Episode Reward=-43.06,\n",
      "Drop Penalty=-31.43,\n",
      "Latency Penalty=0.00,\n",
      "CPU Penalty=0.00,\n",
      "PRB Penalty=0.00,\n",
      "Drop Improvement=-0.79,\n",
      "Latency Improvement=0.00,\n",
      "Energy Efficiency Reward=-3.73,\n",
      "Total Energy Consumption=0.01,\n",
      "Energy Consumption Penalty=-7.12,\n",
      "\n",
      "2025-10-23 15:16:38,771 - PPOAgent - INFO - Epoch 1/8: Actor loss=0.0001, Critic loss=4651.2664\n",
      "2025-10-23 15:16:38,782 - PPOAgent - INFO - Epoch 2/8: Actor loss=-0.0011, Critic loss=4647.3355\n",
      "2025-10-23 15:16:38,794 - PPOAgent - INFO - Epoch 3/8: Actor loss=-0.0032, Critic loss=4643.7779\n",
      "2025-10-23 15:16:38,806 - PPOAgent - INFO - Epoch 4/8: Actor loss=-0.0051, Critic loss=4641.1382\n",
      "2025-10-23 15:16:38,819 - PPOAgent - INFO - Epoch 5/8: Actor loss=-0.0078, Critic loss=4637.5210\n",
      "2025-10-23 15:16:38,832 - PPOAgent - INFO - Epoch 6/8: Actor loss=-0.0105, Critic loss=4633.6613\n",
      "2025-10-23 15:16:38,845 - PPOAgent - INFO - Epoch 7/8: Actor loss=-0.0135, Critic loss=4628.2543\n",
      "2025-10-23 15:16:38,857 - PPOAgent - INFO - Epoch 8/8: Actor loss=-0.0159, Critic loss=4625.7142\n",
      "2025-10-23 15:16:38,857 - PPOAgent - INFO - Training completed: Actor loss=-0.4283, Critic loss=296.7712\n",
      "===================Starting episode: 14===================\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 253, in <module>\n",
      "    main(n_parallel_envs=args.n_envs, scenarios_dir=args.scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 67, in main\n",
      "    results = run_scenario_with_rl_agent_parallel(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_parallel.py\", line 182, in run_scenario_with_rl_agent_parallel\n",
      "    next_obs, rewards, new_dones, infos = envs.step(actions)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simple_parallel_env.py\", line 127, in step\n",
      "    results = [conn.recv() for conn in self.parent_conns]\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simple_parallel_env.py\", line 127, in <listcomp>\n",
      "    results = [conn.recv() for conn in self.parent_conns]\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 250, in recv\n",
      "    buf = self._recv_bytes()\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 414, in _recv_bytes\n",
      "    buf = self._recv(4)\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/multiprocessing/connection.py\", line 379, in _recv\n",
      "    chunk = read(handle, remaining)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('app/energy_agent/config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    num_envs = config['n_envs']\n",
    "\n",
    "!python3 app/main_run_scenarios_parallel.py --n-envs {num_envs} --scenarios-dir app/train_scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: False\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model_6k.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "energy_coeff: 0.01\n",
    "drop_magnitude_penalty_coef: 1.0\n",
    "latency_magnitude_penalty_coef: 0.5\n",
    "cpu_magnitude_penalty_coef: 1.0\n",
    "prb_magnitude_penalty_coef: 1.0\n",
    "improvement_coeff: 0.5\n",
    "\n",
    "violation_event_penalty: -10.0\n",
    "energy_consumption_penalty: -10.0\n",
    "\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 64\n",
    "buffer_size: 2048\n",
    "n_envs: 4\n",
    "hidden_dim: 256\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python3 app/main_run_scenarios_python.py --scenarios-dir app/test_scenarios"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
