{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'RL-for-5G-Energy-Saving' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/nguyenduchuyiu/RL-for-5G-Energy-Saving.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/Project/public_1/5GEnergySaving-Round1-public/RL-for-5G-Energy-Saving\n"
     ]
    }
   ],
   "source": [
    "%cd RL-for-5G-Energy-Saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/energy_agent/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: True\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "qos_grad_coeff: 10.0\n",
    "energy_grad_coeff: 5.0\n",
    "cpu_grad_coeff: 1.0\n",
    "prb_grad_coeff: 1.0\n",
    "stability_penalty: -10.0\n",
    "energy_consumption_penalty: -100.0\n",
    "violation_penalty: -50.0\n",
    "baseline_reward: 10.0\n",
    "\n",
    "entropy_coef: 0.01\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 32\n",
    "buffer_size: 256\n",
    "n_envs: 4\n",
    "hidden_dim: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/dense_urban.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/dense_urban.json\n",
    "{\n",
    "  \"name\": \"Dense Urban\",\n",
    "  \"description\": \"3GPP Dense urban with macro and micro cells\",\n",
    "  \"deploymentScenario\": \"dense_urban\",\n",
    "  \n",
    "  \"carrierFrequency\": 4e9,\n",
    "  \"systemBandwidth\": 200e6,\n",
    "  \"layout\": \"two_layer\",\n",
    "  \"isd\": 200,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"Uniform/macro\",\n",
    "  \"ueSpeed\": 3,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 10,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 2048,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 20,\n",
    "  \"peakHourMultiplier\": 1.3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/urban_macro.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/urban_macro.json\n",
    "    {\n",
    "  \"name\": \"3GPP Urban Macro\",\n",
    "  \"description\": \"3GPP Urban macro deployment scenario with large cells and continuous coverage\",\n",
    "  \"deploymentScenario\": \"urban_macro\",\n",
    "  \n",
    "  \"carrierFrequency\": 2e9,\n",
    "  \"systemBandwidth\": 100e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 500,\n",
    "  \n",
    "  \"numSites\": 7,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 25,\n",
    "  \"cellRadius\": 200,\n",
    "  \n",
    "  \"numUEs\": 300,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 30,\n",
    "  \"indoorRatio\": 0.8,\n",
    "  \"outdoorSpeed\": 30,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1000,\n",
    "  \"idlePower\": 250,\n",
    "  \n",
    "  \"simTime\": 2048,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -110,\n",
    "  \"rsrpTargetThreshold\": -100,\n",
    "  \"rsrpMeasurementThreshold\": -115,\n",
    "  \"dropCallThreshold\": 1,\n",
    "  \"latencyThreshold\": 50,\n",
    "  \"cpuThreshold\": 95,\n",
    "  \"prbThreshold\": 95,\n",
    "  \n",
    "  \"trafficLambda\": 25,\n",
    "  \"peakHourMultiplier\": 1.8\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/train_scenarios/rural.json\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/train_scenarios/rural.json\n",
    "{\n",
    "  \"name\": \"3GPP Rural Macro\",\n",
    "  \"description\": \"3GPP Rural deployment scenario with wide area coverage for high speed vehicles\",\n",
    "  \"deploymentScenario\": \"rural\",\n",
    "  \n",
    "  \"carrierFrequency\": 700e6,\n",
    "  \"systemBandwidth\": 20e6,\n",
    "  \"layout\": \"hexagonal_grid\",\n",
    "  \"isd\": 1732,\n",
    "  \n",
    "  \"numSites\": 19,\n",
    "  \"numSectors\": 3,\n",
    "  \"antennaHeight\": 35,\n",
    "  \"cellRadius\": 1000,\n",
    "  \n",
    "  \"numUEs\": 100,\n",
    "  \"userDistribution\": \"mixed_outdoor_indoor\",\n",
    "  \"ueSpeed\": 120,\n",
    "  \"indoorRatio\": 0.5,\n",
    "  \"outdoorSpeed\": 120,\n",
    "  \n",
    "  \"minTxPower\": 20,\n",
    "  \"maxTxPower\": 46,\n",
    "  \"basePower\": 1200,\n",
    "  \"idlePower\": 300,\n",
    "  \n",
    "  \"simTime\": 2048,\n",
    "  \"timeStep\": 1,\n",
    "  \n",
    "  \"rsrpServingThreshold\": -115,\n",
    "  \"rsrpTargetThreshold\": -105,\n",
    "  \"rsrpMeasurementThreshold\": -120,\n",
    "  \"dropCallThreshold\": 2,\n",
    "  \"latencyThreshold\": 100,\n",
    "  \"cpuThreshold\": 90,\n",
    "  \"prbThreshold\": 90,\n",
    "  \n",
    "  \"trafficLambda\": 10,\n",
    "  \"peakHourMultiplier\": 1.2\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/anaconda3/envs/mira/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "\n",
      "=== Running Benchmark Suite (3 scenarios) ===\n",
      "\n",
      "\n",
      "--- Scenario 1/3: dense_urban ---\n",
      "Loaded scenario: Dense Urban\n",
      "2025-10-28 01:19:12,464 - PPOAgent - INFO - PPO Agent initialized: 21 cells, 300 UEs\n",
      "2025-10-28 01:19:12,465 - PPOAgent - INFO - State dim: 1445, Action dim: 100\n",
      "2025-10-28 01:19:12,465 - PPOAgent - INFO - Device: cuda\n",
      "2025-10-28 01:19:12,484 - PPOAgent - INFO - Model loaded from app/energy_agent/models/ppo_model.pth\n",
      "2025-10-28 01:19:12,484 - PPOAgent - INFO - Loaded checkpoint from app/energy_agent/models/ppo_model.pth\n",
      "Created 7 sites for dense_urban scenario\n",
      "Configuring cells for dense_urban scenario...\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "===================Starting scenario===================\n",
      "Starting episode: 1\n",
      "2025-10-28 01:19:26,670 - PPOAgent - INFO - Training completed: Actor loss=0.2939, Critic loss=11280952.0000, Entropy=91.8938\n",
      "Starting episode: 2\n",
      "2025-10-28 01:19:37,241 - PPOAgent - INFO - Training completed: Actor loss=0.5566, Critic loss=7585727.0000, Entropy=91.8938\n",
      "Starting episode: 3\n",
      "2025-10-28 01:19:48,770 - PPOAgent - INFO - Training completed: Actor loss=0.1410, Critic loss=4832834.0000, Entropy=91.8938\n",
      "Starting episode: 4\n",
      "2025-10-28 01:20:00,006 - PPOAgent - INFO - Training completed: Actor loss=0.1213, Critic loss=2606659.7500, Entropy=91.8938\n",
      "Starting episode: 5\n",
      "2025-10-28 01:20:10,902 - PPOAgent - INFO - Training completed: Actor loss=0.5098, Critic loss=953272.1875, Entropy=91.8938\n",
      "Starting episode: 6\n",
      "2025-10-28 01:20:21,542 - PPOAgent - INFO - Training completed: Actor loss=0.2174, Critic loss=360688.5000, Entropy=91.8938\n",
      "Starting episode: 7\n",
      "2025-10-28 01:20:32,398 - PPOAgent - INFO - Training completed: Actor loss=-0.0000, Critic loss=63249.6484, Entropy=91.8938\n",
      "Starting episode: 8\n",
      "2025-10-28 01:20:43,132 - PPOAgent - INFO - Training completed: Actor loss=-0.0000, Critic loss=4644.6323, Entropy=91.8938\n",
      "Starting episode: 9\n",
      "total_reward:  -1.42857 \n",
      "drop_improvement:  10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -11.42857 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-28 01:20:54,175 - PPOAgent - INFO - Training completed: Actor loss=0.6992, Critic loss=10738.0488, Entropy=91.8938\n",
      "Starting episode: 10\n",
      "2025-10-28 01:21:05,275 - PPOAgent - INFO - Training completed: Actor loss=0.1712, Critic loss=6195.0210, Entropy=91.8938\n",
      "Starting episode: 11\n",
      "total_reward:  -95.63885 \n",
      "drop_improvement:  -2.79346 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.01750 \n",
      "prb_improvement:  0.01750 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -95.38038 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-28 01:21:16,158 - PPOAgent - INFO - Training completed: Actor loss=0.4912, Critic loss=9148.7637, Entropy=91.8938\n",
      "Starting episode: 12\n",
      "2025-10-28 01:21:27,443 - PPOAgent - INFO - Training completed: Actor loss=0.5557, Critic loss=4146.0156, Entropy=91.8938\n",
      "Starting episode: 13\n",
      "Step 50/200: Energy: 0.354 kWh, Power: 25.5 kW, Drop Rate: 1.41%\n",
      "Drop rate violation: 1.41% > 1%\n",
      "2025-10-28 01:21:38,206 - PPOAgent - INFO - Training completed: Actor loss=0.0681, Critic loss=2316.5728, Entropy=91.8938\n",
      "Starting episode: 14\n",
      "2025-10-28 01:21:49,082 - PPOAgent - INFO - Training completed: Actor loss=0.3468, Critic loss=12529.3174, Entropy=91.8938\n",
      "Starting episode: 15\n",
      "2025-10-28 01:22:00,478 - PPOAgent - INFO - Training completed: Actor loss=0.0934, Critic loss=3379.3345, Entropy=91.8938\n",
      "Starting episode: 16\n",
      "2025-10-28 01:22:11,665 - PPOAgent - INFO - Training completed: Actor loss=0.1326, Critic loss=1105.9535, Entropy=91.8938\n",
      "Starting episode: 17\n",
      "total_reward:  -45.78353 \n",
      "drop_improvement:  -0.71433 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -45.06920 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-28 01:22:22,773 - PPOAgent - INFO - Training completed: Actor loss=0.5807, Critic loss=531.8052, Entropy=91.8938\n",
      "Starting episode: 18\n",
      "2025-10-28 01:22:32,925 - PPOAgent - INFO - Training completed: Actor loss=0.2237, Critic loss=8293.3213, Entropy=91.8938\n",
      "Starting episode: 19\n",
      "2025-10-28 01:22:43,573 - PPOAgent - INFO - Training completed: Actor loss=0.1122, Critic loss=1387.2771, Entropy=91.8938\n",
      "Starting episode: 20\n",
      "2025-10-28 01:22:54,288 - PPOAgent - INFO - Training completed: Actor loss=-0.0000, Critic loss=6800.7446, Entropy=91.8938\n",
      "Starting episode: 21\n",
      "2025-10-28 01:23:05,077 - PPOAgent - INFO - Training completed: Actor loss=0.7923, Critic loss=2228.9604, Entropy=91.8938\n",
      "Starting episode: 22\n",
      "2025-10-28 01:23:15,363 - PPOAgent - INFO - Training completed: Actor loss=-0.0000, Critic loss=3661.9526, Entropy=91.8938\n",
      "Starting episode: 23\n",
      "2025-10-28 01:23:26,318 - PPOAgent - INFO - Training completed: Actor loss=-0.0000, Critic loss=12622.0996, Entropy=91.8938\n",
      "Starting episode: 24\n",
      "2025-10-28 01:23:36,528 - PPOAgent - INFO - Training completed: Actor loss=0.5399, Critic loss=10663.1562, Entropy=91.8938\n",
      "Starting episode: 25\n",
      "Step 100/200: Energy: 0.707 kWh, Power: 25.7 kW, Drop Rate: 1.03%\n",
      "Drop rate violation: 1.03% > 1%\n",
      "2025-10-28 01:23:47,149 - PPOAgent - INFO - Training completed: Actor loss=0.6963, Critic loss=4847.3755, Entropy=91.8938\n",
      "Starting episode: 26\n",
      "total_reward:  -109.11205 \n",
      "drop_improvement:  -10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -99.11205 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  0.00000\n",
      "2025-10-28 01:23:58,494 - PPOAgent - INFO - Training completed: Actor loss=0.5893, Critic loss=12044.2090, Entropy=91.8938\n",
      "Starting episode: 27\n",
      "2025-10-28 01:24:09,382 - PPOAgent - INFO - Training completed: Actor loss=0.7397, Critic loss=8681.1396, Entropy=91.8938\n",
      "Starting episode: 28\n",
      "2025-10-28 01:24:20,392 - PPOAgent - INFO - Training completed: Actor loss=0.0630, Critic loss=7113.0654, Entropy=91.8938\n",
      "Starting episode: 29\n",
      "total_reward:  -68.01440 \n",
      "drop_improvement:  -10.00000 \n",
      "latency_improvement:  0.00000 \n",
      "cpu_improvement:  0.00000 \n",
      "prb_improvement:  0.00000 \n",
      "energy_efficiency_reward:  0.00000 \n",
      "stability_penalty:  0.00000 \n",
      "violation_penalty:  -60.51440 \n",
      "energy_consumption_penalty:  0.00000 \n",
      "warning_reward:  2.50000\n",
      "2025-10-28 01:24:31,959 - PPOAgent - INFO - Training completed: Actor loss=-0.0000, Critic loss=910.5500, Entropy=91.8938\n",
      "Starting episode: 30\n",
      "2025-10-28 01:24:42,787 - PPOAgent - INFO - Training completed: Actor loss=0.0550, Critic loss=785.0682, Entropy=91.8938\n",
      "Starting episode: 31\n",
      "2025-10-28 01:24:54,056 - PPOAgent - INFO - Training completed: Actor loss=0.2152, Critic loss=5379.5010, Entropy=91.8938\n",
      "Starting episode: 32\n",
      "2025-10-28 01:25:04,617 - PPOAgent - INFO - Training completed: Actor loss=0.1189, Critic loss=890.7234, Entropy=91.8938\n",
      "Starting episode: 33\n",
      "2025-10-28 01:25:15,904 - PPOAgent - INFO - Training completed: Actor loss=0.1174, Critic loss=163.3317, Entropy=91.8938\n",
      "Starting episode: 34\n",
      "2025-10-28 01:25:26,474 - PPOAgent - INFO - Training completed: Actor loss=0.0956, Critic loss=4590.6016, Entropy=91.8938\n",
      "Starting episode: 35\n",
      "2025-10-28 01:25:36,891 - PPOAgent - INFO - Training completed: Actor loss=0.5737, Critic loss=9263.2070, Entropy=91.8938\n",
      "Starting episode: 36\n",
      "2025-10-28 01:25:47,372 - PPOAgent - INFO - Training completed: Actor loss=0.6751, Critic loss=2062.0137, Entropy=91.8938\n",
      "Starting episode: 37\n",
      "2025-10-28 01:25:58,307 - PPOAgent - INFO - Training completed: Actor loss=0.0002, Critic loss=188.9578, Entropy=91.8938\n",
      "Starting episode: 38\n",
      "Step 150/200: Energy: 1.062 kWh, Power: 25.6 kW, Drop Rate: 0.96%\n",
      "2025-10-28 01:26:09,692 - PPOAgent - INFO - Training completed: Actor loss=0.0250, Critic loss=3426.8616, Entropy=91.8938\n",
      "Starting episode: 39\n",
      "2025-10-28 01:26:20,846 - PPOAgent - INFO - Training completed: Actor loss=0.2135, Critic loss=2647.5610, Entropy=91.8938\n",
      "Starting episode: 40\n",
      "2025-10-28 01:26:31,602 - PPOAgent - INFO - Training completed: Actor loss=0.5887, Critic loss=8083.8340, Entropy=91.8938\n",
      "Starting episode: 41\n",
      "2025-10-28 01:26:42,402 - PPOAgent - INFO - Training completed: Actor loss=0.0222, Critic loss=970.1901, Entropy=91.8938\n",
      "Starting episode: 42\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_python.py\", line 172, in <module>\n",
      "    main(scenarios_dir=args.scenarios_dir, base_seed=args.base_seed)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_python.py\", line 61, in main\n",
      "    results = run_scenario_with_rl_agent(scenario=name, seed=seed, scenarios_dir=scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_python.py\", line 132, in run_scenario_with_rl_agent\n",
      "    next_state, reward, done, truncated, info = env.step(action)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simulation/environment.py\", line 118, in step\n",
      "    update_signal_measurements(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simulation/signal_measurement.py\", line 36, in update_signal_measurements\n",
      "    path_loss_matrix = calculate_path_loss_vectorized(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simulation/path_loss.py\", line 99, in calculate_path_loss_vectorized\n",
      "    rng = np.random.RandomState(pl_seed)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "import yaml\n",
    "with open('app/energy_agent/config.yaml') as f:\n",
    "    config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "    num_envs = config['n_envs']\n",
    "\n",
    "!python3 app/main_run_scenarios_python.py --scenarios-dir app/train_scenarios"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting app/energy_agent/config.yaml\n"
     ]
    }
   ],
   "source": [
    "%%writefile app/energy_agent/config.yaml\n",
    "training_mode: False\n",
    "use_gpu: True\n",
    "checkpoint_path: \"app/energy_agent/models/ppo_model.pth\"\n",
    "\n",
    "actor_lr: 0.0001\n",
    "critic_lr: 0.0003\n",
    "\n",
    "qos_grad_coeff: 10.0\n",
    "energy_grad_coeff: 5.0\n",
    "cpu_grad_coeff: 1.0\n",
    "prb_grad_coeff: 1.0\n",
    "stability_penalty: 2.0\n",
    "energy_consumption_penalty: 1000.0\n",
    "\n",
    "entropy_coef: 0.01\n",
    "gamma: 0.99\n",
    "lambda_gae: 0.95\n",
    "clip_epsilon: 0.2\n",
    "ppo_epochs: 8\n",
    "batch_size: 2\n",
    "buffer_size: 4\n",
    "n_envs: 4\n",
    "hidden_dim: 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/huy/anaconda3/envs/mira/lib/python3.10/site-packages/torch/cuda/__init__.py:61: FutureWarning: The pynvml package is deprecated. Please install nvidia-ml-py instead. If you did not install pynvml directly, please report this to the maintainers of the package that installed pynvml for you.\n",
      "  import pynvml  # type: ignore[import]\n",
      "\n",
      "=== Running Benchmark Suite (3 scenarios) ===\n",
      "\n",
      "\n",
      "--- Scenario 1/3: dense_urban ---\n",
      "Loaded scenario: Dense Urban\n",
      "2025-10-28 01:28:35,257 - PPOAgent - INFO - PPO Agent initialized: 21 cells, 300 UEs\n",
      "2025-10-28 01:28:35,257 - PPOAgent - INFO - State dim: 1445, Action dim: 100\n",
      "2025-10-28 01:28:35,257 - PPOAgent - INFO - Device: cuda\n",
      "2025-10-28 01:28:35,257 - PPOAgent - INFO - No checkpoint found at app/energy_agent/models/ppo_model.pth\n",
      "Created 7 sites for dense_urban scenario\n",
      "Configuring cells for dense_urban scenario...\n",
      "Configured 21 cells for dense_urban scenario\n",
      "Initialized 300 UEs for dense_urban scenario\n",
      "===================Starting scenario===================\n",
      "Starting episode: 1\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_python.py\", line 172, in <module>\n",
      "    main(scenarios_dir=args.scenarios_dir, base_seed=args.base_seed)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_python.py\", line 61, in main\n",
      "    results = run_scenario_with_rl_agent(scenario=name, seed=seed, scenarios_dir=scenarios_dir)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/main_run_scenarios_python.py\", line 132, in run_scenario_with_rl_agent\n",
      "    next_state, reward, done, truncated, info = env.step(action)\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simulation/environment.py\", line 118, in step\n",
      "    update_signal_measurements(\n",
      "  File \"/home/huy/Project/public_1/5GEnergySaving-Round1-public/app/simulation/signal_measurement.py\", line 71, in update_signal_measurements\n",
      "    rng = np.random.RandomState(rsrp_seed)\n",
      "  File \"numpy/random/mtrand.pyx\", line 185, in numpy.random.mtrand.RandomState.__init__\n",
      "  File \"_mt19937.pyx\", line 132, in numpy.random._mt19937.MT19937.__init__\n",
      "  File \"/home/huy/anaconda3/envs/mira/lib/python3.10/site-packages/numpy/_core/_ufunc_config.py\", line 479, in inner\n",
      "    return func(*args, **kwargs)\n",
      "KeyboardInterrupt\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!python3 app/main_run_scenarios_python.py --scenarios-dir app/test_scenarios"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "mira",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
